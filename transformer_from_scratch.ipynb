{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "htzdzthTrxJU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix basics with PyTorch"
      ],
      "metadata": {
        "id": "bd1usgCwDpNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.tensor() has type torch.int64\n",
        "print(torch.tensor([[[1,2,3], [3,4,5]]]))\n",
        "\n",
        "# torch.Tensor() has type torch.float32\n",
        "print(torch.Tensor([[[1,2,3], [3,4,5]]]))\n",
        "\n",
        "# torch.IntTensor() has type torch.int32\n",
        "print(torch.IntTensor([[[1,2,3], [3,4,5]]]))\n",
        "\n",
        "# torch.tensor() can be forced to have different type, e.g. torch.float32\n",
        "print(torch.tensor([[[1,2,3], [3,4,5]]], dtype=torch.float32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2wly5M3EXcP",
        "outputId": "dca1cd03-8afa-4262-e92d-203c48b991aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1, 2, 3],\n",
            "         [3, 4, 5]]])\n",
            "tensor([[[1., 2., 3.],\n",
            "         [3., 4., 5.]]])\n",
            "tensor([[[1, 2, 3],\n",
            "         [3, 4, 5]]], dtype=torch.int32)\n",
            "tensor([[[1., 2., 3.],\n",
            "         [3., 4., 5.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix multiplication\n",
        "\n",
        "A = torch.tensor([[[1,2,3], [3,4,5]]])\n",
        "print(f\"Matrix A: \\n{A}\")\n",
        "print(f\"Size of matrix A: {A.size()}\\n\")\n",
        "\n",
        "B = torch.tensor([[[1,2], [3,4], [4, 5]]])\n",
        "print(f\"Matrix B: \\n{B}\")\n",
        "print(f\"Size of matrix B: {B.size()}\\n\")\n",
        "\n",
        "print(f\"Matrix multiplication: \\n{torch.matmul(A,B)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew5eqHk_DrlK",
        "outputId": "895badf8-6985-4d55-b64d-77554c645256"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A: \n",
            "tensor([[[1, 2, 3],\n",
            "         [3, 4, 5]]])\n",
            "Size of matrix A: torch.Size([1, 2, 3])\n",
            "\n",
            "Matrix B: \n",
            "tensor([[[1, 2],\n",
            "         [3, 4],\n",
            "         [4, 5]]])\n",
            "Size of matrix B: torch.Size([1, 3, 2])\n",
            "\n",
            "Matrix multiplication: \n",
            "tensor([[[19, 25],\n",
            "         [35, 47]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose matrix\n",
        "\n",
        "A = torch.rand(1, 4, 8)\n",
        "print(f\"Original matrix A: \\n{A}\")\n",
        "\n",
        "A = A.transpose(1,2)\n",
        "print(f\"Transposed matrix A: \\n{A}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kufovrd-FKZT",
        "outputId": "f1009102-2ec8-47cc-ba97-be41b198e762"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original matrix A: \n",
            "tensor([[[0.3243, 0.8374, 0.4727, 0.5031, 0.9098, 0.9627, 0.0801, 0.6658],\n",
            "         [0.9209, 0.2973, 0.6347, 0.1612, 0.9589, 0.8879, 0.1335, 0.7338],\n",
            "         [0.1296, 0.2361, 0.6139, 0.2943, 0.0774, 0.0776, 0.9493, 0.3559],\n",
            "         [0.4670, 0.6582, 0.8039, 0.7059, 0.7350, 0.2305, 0.7733, 0.2713]]])\n",
            "Transposed matrix A: \n",
            "tensor([[[0.3243, 0.9209, 0.1296, 0.4670],\n",
            "         [0.8374, 0.2973, 0.2361, 0.6582],\n",
            "         [0.4727, 0.6347, 0.6139, 0.8039],\n",
            "         [0.5031, 0.1612, 0.2943, 0.7059],\n",
            "         [0.9098, 0.9589, 0.0774, 0.7350],\n",
            "         [0.9627, 0.8879, 0.0776, 0.2305],\n",
            "         [0.0801, 0.1335, 0.9493, 0.7733],\n",
            "         [0.6658, 0.7338, 0.3559, 0.2713]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Masking a matrix\n",
        "\n",
        "A = torch.rand(1, 8, 8)\n",
        "print(f\"Original matrix A: \\n{A}\\n\")\n",
        "\n",
        "# Apply a mask to prevent attending to future positions\n",
        "mask = torch.triu(torch.ones_like(A), diagonal=1)\n",
        "print(f\"Mask matrix: \\n{mask}\\n\")\n",
        "\n",
        "masked_A = A.masked_fill(mask == 1, float('-inf'))\n",
        "print(f\"Masked matrix A: \\n{masked_A}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g61Dys0cxc1T",
        "outputId": "41a7e02d-1a12-4154-f888-15304af4b7e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original matrix A: \n",
            "tensor([[[0.2690, 0.2443, 0.8863, 0.6014, 0.9206, 0.4244, 0.3763, 0.0589],\n",
            "         [0.9289, 0.0930, 0.4645, 0.5099, 0.6596, 0.3787, 0.6062, 0.2162],\n",
            "         [0.9220, 0.5900, 0.3197, 0.5098, 0.8085, 0.7519, 0.4998, 0.2204],\n",
            "         [0.3446, 0.9127, 0.8496, 0.6827, 0.9369, 0.3711, 0.0767, 0.5201],\n",
            "         [0.5098, 0.7619, 0.9631, 0.6053, 0.5830, 0.2460, 0.5494, 0.7003],\n",
            "         [0.5261, 0.6772, 0.7098, 0.3315, 0.7657, 0.6921, 0.8105, 0.5333],\n",
            "         [0.1105, 0.8428, 0.6619, 0.2209, 0.9473, 0.1185, 0.7771, 0.2293],\n",
            "         [0.7675, 0.8342, 0.4826, 0.0350, 0.3600, 0.0227, 0.8604, 0.3529]]])\n",
            "\n",
            "Mask matrix: \n",
            "tensor([[[0., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
            "         [0., 0., 0., 1., 1., 1., 1., 1.],\n",
            "         [0., 0., 0., 0., 1., 1., 1., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 1., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "\n",
            "Masked matrix A: \n",
            "tensor([[[0.2690,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "         [0.9289, 0.0930,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "         [0.9220, 0.5900, 0.3197,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "         [0.3446, 0.9127, 0.8496, 0.6827,   -inf,   -inf,   -inf,   -inf],\n",
            "         [0.5098, 0.7619, 0.9631, 0.6053, 0.5830,   -inf,   -inf,   -inf],\n",
            "         [0.5261, 0.6772, 0.7098, 0.3315, 0.7657, 0.6921,   -inf,   -inf],\n",
            "         [0.1105, 0.8428, 0.6619, 0.2209, 0.9473, 0.1185, 0.7771,   -inf],\n",
            "         [0.7675, 0.8342, 0.4826, 0.0350, 0.3600, 0.0227, 0.8604, 0.3529]]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Attention (scaled dot-product attention)"
      ],
      "metadata": {
        "id": "tImGhjBrDjuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, masked: bool = False):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.masked = masked\n",
        "\n",
        "    def forward(self, Q: torch.tensor, K: torch.tensor, V: torch.tensor) -> torch.tensor:\n",
        "        d_k = K.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(1, 2)) / (d_k ** 0.5)\n",
        "\n",
        "        if self.masked:\n",
        "            print(f\"Unmasked scores: \\n{scores}\\n\")\n",
        "            mask = torch.triu(torch.ones_like(scores), diagonal=1)\n",
        "            scores = scores.masked_fill(mask == 1, float('-inf'))\n",
        "            print(f\"Mask for scores: \\n{mask}\\n\")\n",
        "            print(f\"Masked attention scores: \\n{scores}\\n\")\n",
        "\n",
        "        norm_scores = F.softmax(scores, -1)\n",
        "        print(f\"Normalized attention scores: \\n{norm_scores}\\n\")\n",
        "\n",
        "        attention = torch.matmul(norm_scores, V)\n",
        "        return attention"
      ],
      "metadata": {
        "id": "x_Xdt5n2rxLn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage example"
      ],
      "metadata": {
        "id": "zuMtnJDPuhgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1  # number of sequences\n",
        "seq_length = 4  # n_elements of the sequence\n",
        "embed_size = 7  # dimension of the vectors representing the elements\n",
        "\n",
        "# Generate a random sequence of 4 elements, each one represented by a 7-dim vector\n",
        "sequence_in_matrix_form = torch.rand(batch_size, seq_length, embed_size)\n",
        "print(f\"Representation of the sequence: \\n{sequence_in_matrix_form}\\n\")\n",
        "\n",
        "# Arrange the sequence in query Q, key K, and value V matrices\n",
        "Q = K = V = sequence_in_matrix_form\n",
        "\n",
        "# Self Attention\n",
        "self_attention = SelfAttention()\n",
        "attention = self_attention(Q, K, V)\n",
        "print(f\"More contextualized representations (attention): \\n{attention}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OE9Lx-1vMEF",
        "outputId": "d47965b9-0c19-4122-a703-777bab6ea727"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representation of the sequence: \n",
            "tensor([[[0.9825, 0.2555, 0.2238, 0.1468, 0.5624, 0.7028, 0.1377],\n",
            "         [0.3278, 0.5121, 0.9508, 0.1400, 0.8237, 0.1812, 0.6046],\n",
            "         [0.7061, 0.2739, 0.3204, 0.6168, 0.8592, 0.9566, 0.4810],\n",
            "         [0.4378, 0.1718, 0.8191, 0.4867, 0.3966, 0.6232, 0.1440]]])\n",
            "\n",
            "Normalized attention scores: \n",
            "tensor([[[0.2694, 0.2171, 0.2923, 0.2212],\n",
            "         [0.2081, 0.3049, 0.2586, 0.2283],\n",
            "         [0.2386, 0.2201, 0.3220, 0.2193],\n",
            "         [0.2270, 0.2444, 0.2758, 0.2529]]])\n",
            "\n",
            "More contextualized representations (attention): \n",
            "tensor([[[0.6391, 0.2981, 0.5416, 0.3579, 0.6692, 0.6462, 0.3408],\n",
            "         [0.5870, 0.3194, 0.6064, 0.3439, 0.6810, 0.5912, 0.3703],\n",
            "         [0.6299, 0.2996, 0.5455, 0.3712, 0.6791, 0.6523, 0.3524],\n",
            "         [0.6086, 0.3021, 0.5787, 0.3607, 0.6662, 0.6252, 0.3481]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Masked Self Attention\n",
        "self_attention = SelfAttention(masked=True)\n",
        "attention = self_attention(Q, K, V)\n",
        "print(f\"More contextualized representations (attention): \\n{attention}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q74vN-KpyyN5",
        "outputId": "7857c20d-2708-4262-91d7-e5cdac4da0c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unmasked scores: \n",
            "tensor([[[0.7300, 0.5141, 0.8118, 0.5328],\n",
            "         [0.5141, 0.8959, 0.7312, 0.6067],\n",
            "         [0.8118, 0.7312, 1.1118, 0.7276],\n",
            "         [0.5328, 0.6067, 0.7276, 0.6409]]])\n",
            "\n",
            "Mask for scores: \n",
            "tensor([[[0., 1., 1., 1.],\n",
            "         [0., 0., 1., 1.],\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "\n",
            "Masked attention scores: \n",
            "tensor([[[0.7300,   -inf,   -inf,   -inf],\n",
            "         [0.5141, 0.8959,   -inf,   -inf],\n",
            "         [0.8118, 0.7312, 1.1118,   -inf],\n",
            "         [0.5328, 0.6067, 0.7276, 0.6409]]])\n",
            "\n",
            "Normalized attention scores: \n",
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4057, 0.5943, 0.0000, 0.0000],\n",
            "         [0.3056, 0.2819, 0.4125, 0.0000],\n",
            "         [0.2270, 0.2444, 0.2758, 0.2529]]])\n",
            "\n",
            "More contextualized representations (attention): \n",
            "tensor([[[0.9825, 0.2555, 0.2238, 0.1468, 0.5624, 0.7028, 0.1377],\n",
            "         [0.5934, 0.4080, 0.6559, 0.1428, 0.7177, 0.3928, 0.4152],\n",
            "         [0.6839, 0.3355, 0.4686, 0.3388, 0.7585, 0.6604, 0.4109],\n",
            "         [0.6086, 0.3021, 0.5787, 0.3607, 0.6662, 0.6252, 0.3481]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head Attention"
      ],
      "metadata": {
        "id": "ojl1KINSGfTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads: int = 6, d_model: int = 512, masked: bool = False):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.attention_mechanism = SelfAttention(masked=masked)\n",
        "\n",
        "        # Linear projection for {Q, K, V}\n",
        "        self.W_Q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.W_K = nn.Linear(self.d_model, self.d_model)\n",
        "        self.W_V = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # Linear projection for the concatenated attentions\n",
        "        self.W_O = nn.Linear(self.d_model * self.n_heads, self.d_model)\n",
        "\n",
        "    def forward(self, Q: torch.tensor, K: torch.tensor, V: torch.tensor) -> torch.tensor:\n",
        "        batch_size = Q.size(0)\n",
        "        seq_length = Q.size(1)\n",
        "        assert(Q.size(2) == self.d_model), f\"Error: the embedding space is not {self.d_model}-dim\"\n",
        "\n",
        "        concatenated_attentions = torch.empty(0)\n",
        "\n",
        "        for i in range(self.n_heads):\n",
        "            proj_Q = self.W_Q(Q)\n",
        "            proj_K = self.W_K(K)\n",
        "            proj_V = self.W_V(V)\n",
        "\n",
        "            attention = self.attention_mechanism(proj_Q, proj_K, proj_V)\n",
        "            print(f\"Attention at iteration {i}: \\n{attention}\")\n",
        "\n",
        "            concatenated_attentions = torch.cat([concatenated_attentions, attention], dim=-1)\n",
        "            print(f\"Concatenated attentions size: {concatenated_attentions.size()}\\n\")\n",
        "\n",
        "        return self.W_O(concatenated_attentions)"
      ],
      "metadata": {
        "id": "iIVZTmUuxxa7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage example"
      ],
      "metadata": {
        "id": "Z7Lx_y1qumJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1  # number of sequences\n",
        "seq_length = 4  # n_elements of the sequence\n",
        "embed_size = 7  # dimension of the vectors representing the elements\n",
        "\n",
        "# Generate a random sequence of 4 elements, each one represented by a 4-dim vector\n",
        "sequence_in_matrix_form = torch.rand(batch_size, seq_length, embed_size)\n",
        "print(f\"Representation of the sequence: \\n{sequence_in_matrix_form}\\n\")\n",
        "\n",
        "# Arrange the sequence in query Q, key K, and value V matrices\n",
        "Q = K = V = sequence_in_matrix_form\n",
        "\n",
        "# Multi-head Attention\n",
        "n_heads = 3\n",
        "d_model = embed_size\n",
        "multihead_attention = MultiHeadAttention(n_heads=n_heads, d_model=d_model)\n",
        "attention = multihead_attention(Q, K, V)\n",
        "print(f\"More contextualized representations (attention): \\n{attention}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfTIxJqOY3-z",
        "outputId": "ad05fae8-0a58-4228-f91b-46d7e62d3dd8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representation of the sequence: \n",
            "tensor([[[0.8717, 0.9598, 0.9938, 0.5128, 0.2015, 0.6769, 0.4465],\n",
            "         [0.3456, 0.2355, 0.6558, 0.2093, 0.5591, 0.6205, 0.0138],\n",
            "         [0.8174, 0.1511, 0.8926, 0.7813, 0.9035, 0.2674, 0.6150],\n",
            "         [0.6492, 0.8087, 0.5632, 0.8990, 0.8388, 0.8483, 0.8361]]])\n",
            "\n",
            "Attention at iteration 0: \n",
            "tensor([[[ 0.2806, -0.5223,  0.9833, -0.4471,  0.4553, -0.4384, -0.2859],\n",
            "         [ 0.2737, -0.5187,  0.9791, -0.4487,  0.4544, -0.4353, -0.2828],\n",
            "         [ 0.2733, -0.5184,  0.9769, -0.4500,  0.4525, -0.4330, -0.2838],\n",
            "         [ 0.2832, -0.5219,  0.9821, -0.4479,  0.4537, -0.4359, -0.2866]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Concatenated attentions size: torch.Size([1, 4, 7])\n",
            "\n",
            "Attention at iteration 1: \n",
            "tensor([[[ 0.2806, -0.5223,  0.9833, -0.4471,  0.4553, -0.4384, -0.2859],\n",
            "         [ 0.2737, -0.5187,  0.9791, -0.4487,  0.4544, -0.4353, -0.2828],\n",
            "         [ 0.2733, -0.5184,  0.9769, -0.4500,  0.4525, -0.4330, -0.2838],\n",
            "         [ 0.2832, -0.5219,  0.9821, -0.4479,  0.4537, -0.4359, -0.2866]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Concatenated attentions size: torch.Size([1, 4, 14])\n",
            "\n",
            "Attention at iteration 2: \n",
            "tensor([[[ 0.2806, -0.5223,  0.9833, -0.4471,  0.4553, -0.4384, -0.2859],\n",
            "         [ 0.2737, -0.5187,  0.9791, -0.4487,  0.4544, -0.4353, -0.2828],\n",
            "         [ 0.2733, -0.5184,  0.9769, -0.4500,  0.4525, -0.4330, -0.2838],\n",
            "         [ 0.2832, -0.5219,  0.9821, -0.4479,  0.4537, -0.4359, -0.2866]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Concatenated attentions size: torch.Size([1, 4, 21])\n",
            "\n",
            "More contextualized representations (attention): \n",
            "tensor([[[-0.1790,  0.1167, -0.1578, -0.6456, -0.0838, -0.2019,  0.1744],\n",
            "         [-0.1775,  0.1167, -0.1602, -0.6401, -0.0842, -0.1996,  0.1748],\n",
            "         [-0.1775,  0.1167, -0.1599, -0.6386, -0.0844, -0.1987,  0.1751],\n",
            "         [-0.1794,  0.1168, -0.1567, -0.6452, -0.0841, -0.2012,  0.1738]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Masked Multi-head Attention\n",
        "n_heads = 3\n",
        "d_model = embed_size\n",
        "\n",
        "multihead_attention = MultiHeadAttention(n_heads=n_heads, d_model=d_model)\n",
        "attention = multihead_attention(Q, K, V)\n",
        "print(f\"More contextualized representations (attention): \\n{attention}\")"
      ],
      "metadata": {
        "id": "S-PFPed_yqSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b41f83d-56c6-4a7b-9f9f-0def0edb02f2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention at iteration 0: \n",
            "tensor([[[-0.3396, -0.1981, -0.4887,  0.3654, -0.2124,  0.7122, -0.6723],\n",
            "         [-0.3471, -0.1970, -0.4955,  0.3595, -0.2115,  0.7207, -0.6731],\n",
            "         [-0.3438, -0.1983, -0.4937,  0.3639, -0.2151,  0.7189, -0.6728],\n",
            "         [-0.3385, -0.1978, -0.4884,  0.3667, -0.2138,  0.7124, -0.6719]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Concatenated attentions size: torch.Size([1, 4, 7])\n",
            "\n",
            "Attention at iteration 1: \n",
            "tensor([[[-0.3396, -0.1981, -0.4887,  0.3654, -0.2124,  0.7122, -0.6723],\n",
            "         [-0.3471, -0.1970, -0.4955,  0.3595, -0.2115,  0.7207, -0.6731],\n",
            "         [-0.3438, -0.1983, -0.4937,  0.3639, -0.2151,  0.7189, -0.6728],\n",
            "         [-0.3385, -0.1978, -0.4884,  0.3667, -0.2138,  0.7124, -0.6719]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Concatenated attentions size: torch.Size([1, 4, 14])\n",
            "\n",
            "Attention at iteration 2: \n",
            "tensor([[[-0.3396, -0.1981, -0.4887,  0.3654, -0.2124,  0.7122, -0.6723],\n",
            "         [-0.3471, -0.1970, -0.4955,  0.3595, -0.2115,  0.7207, -0.6731],\n",
            "         [-0.3438, -0.1983, -0.4937,  0.3639, -0.2151,  0.7189, -0.6728],\n",
            "         [-0.3385, -0.1978, -0.4884,  0.3667, -0.2138,  0.7124, -0.6719]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Concatenated attentions size: torch.Size([1, 4, 21])\n",
            "\n",
            "More contextualized representations (attention): \n",
            "tensor([[[ 0.0570, -0.3767, -0.7968,  0.2286,  0.4367,  0.1082, -0.2106],\n",
            "         [ 0.0547, -0.3753, -0.8012,  0.2311,  0.4362,  0.1097, -0.2086],\n",
            "         [ 0.0560, -0.3778, -0.8005,  0.2311,  0.4380,  0.1097, -0.2090],\n",
            "         [ 0.0575, -0.3777, -0.7966,  0.2286,  0.4373,  0.1086, -0.2104]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "xVhX_M8PkZI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads: int = 6, d_model: int = 512):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Multi-head attention sub-layer\n",
        "        self.multihead_attention = MultiHeadAttention(n_heads, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Feed-forward sub-layer\n",
        "        self.feedforward_1 = nn.Linear(d_model, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.feedforward_2 = nn.Linear(d_model, d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, X: torch.tensor) -> torch.tensor:\n",
        "\n",
        "        # Multi-head attention sub-layer\n",
        "        attention = self.multihead_attention(X, X, X)\n",
        "        X = X + attention\n",
        "        X = self.norm1(X)\n",
        "\n",
        "        # Feed-forward sub-Layer\n",
        "        feedforward_output = self.feedforward_1(X)\n",
        "        feedforward_output = self.relu(feedforward_output)\n",
        "        feedforward_output = self.feedforward_2(feedforward_output)\n",
        "        X = X + feedforward_output\n",
        "        X = self.norm2(X)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "hdzGs2DqlFkF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_layers: int = 6, n_heads: int = 6, d_model: int = 512):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.encoder_layer = EncoderLayer(n_heads, d_model)\n",
        "\n",
        "    def forward(self, X: torch.tensor) -> torch.tensor:\n",
        "        for i in range(self.n_layers):\n",
        "            X = self.encoder_layer(X)\n",
        "            print(f\"Representation at stage {i}: \\n{X}\\n\")\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "mpl1hpl5nExC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage example"
      ],
      "metadata": {
        "id": "knKqzaeYrfqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "seq_length = 4\n",
        "embed_size = 7\n",
        "sequence_in_matrix_form = torch.rand(batch_size, seq_length, embed_size)\n",
        "print(f\"Representation of the sequence: \\n{sequence_in_matrix_form}\\n\")\n",
        "\n",
        "# Encoder\n",
        "n_layers = 2\n",
        "n_heads = 3\n",
        "d_model = embed_size\n",
        "encoder = Encoder(n_layers=n_layers, n_heads=n_heads, d_model=d_model)\n",
        "encoder_output = encoder(sequence_in_matrix_form)\n",
        "print(f\"Encoder representations: \\n{encoder_output}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz3iD9xargcp",
        "outputId": "0d42d91c-bf63-4355-b62c-8c22e1f065d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representation of the sequence: \n",
            "tensor([[[0.3730, 0.1447, 0.9125, 0.5586, 0.7675, 0.0929, 0.0384],\n",
            "         [0.6887, 0.3913, 0.8399, 0.4751, 0.2218, 0.5934, 0.1503],\n",
            "         [0.2194, 0.7998, 0.2934, 0.9579, 0.0916, 0.5234, 0.2790],\n",
            "         [0.1715, 0.1624, 0.3647, 0.0395, 0.4536, 0.3467, 0.5838]]])\n",
            "\n",
            "Representation at stage 0: \n",
            "tensor([[[-0.8053, -0.6211,  1.0239,  1.3540,  1.0333, -0.8404, -1.1445],\n",
            "         [-0.1917, -0.7871,  1.0643,  1.6046, -0.6106,  0.3768, -1.4564],\n",
            "         [-1.1909,  0.4318, -0.3702,  2.1193, -0.4493,  0.1873, -0.7280],\n",
            "         [-1.8847, -1.1253,  0.1592,  0.4080,  0.7934,  0.8247,  0.8247]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Representation at stage 1: \n",
            "tensor([[[-1.1418, -0.3174,  0.6351,  1.7221,  0.8430, -0.8848, -0.8562],\n",
            "         [-0.7175, -0.5555,  0.6273,  2.1049, -0.3830,  0.0217, -1.0979],\n",
            "         [-1.3255,  0.3294, -0.4436,  2.1490, -0.2360,  0.0387, -0.5119],\n",
            "         [-2.0854, -0.6173, -0.2371,  0.8769,  0.5547,  0.7497,  0.7585]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Encoder representations: \n",
            "tensor([[[-1.1418, -0.3174,  0.6351,  1.7221,  0.8430, -0.8848, -0.8562],\n",
            "         [-0.7175, -0.5555,  0.6273,  2.1049, -0.3830,  0.0217, -1.0979],\n",
            "         [-1.3255,  0.3294, -0.4436,  2.1490, -0.2360,  0.0387, -0.5119],\n",
            "         [-2.0854, -0.6173, -0.2371,  0.8769,  0.5547,  0.7497,  0.7585]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "Fl7htIZOkiqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads: int = 6, d_model: int = 512):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Masked multi-head attention sub-layer\n",
        "        self.masked_multihead_attention = MultiHeadAttention(n_heads, d_model, masked=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Multi-head attention sub-layer\n",
        "        self.multihead_attention = MultiHeadAttention(n_heads, d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Feed-forward sub-layer\n",
        "        self.feedforward_1 = nn.Linear(d_model, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.feedforward_2 = nn.Linear(d_model, d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, X: torch.tensor, encoder_output: torch.tensor) -> torch.tensor:\n",
        "\n",
        "        # Masked multi-head attention sub-layer\n",
        "        masked_attention = self.masked_multihead_attention(X, X, X)\n",
        "        X = X + masked_attention\n",
        "        X = self.norm1(X)\n",
        "\n",
        "        # Multi-head attention sub-layer with encoder output\n",
        "        attention_output = self.multihead_attention(X, encoder_output, encoder_output)\n",
        "        X = X + attention_output\n",
        "        X = self.norm2(X)\n",
        "\n",
        "        # Feed-forward sub-Layer\n",
        "        feedforward_output = self.feedforward_1(X)\n",
        "        feedforward_output = self.relu(feedforward_output)\n",
        "        feedforward_output = self.feedforward_2(feedforward_output)\n",
        "        X = X + feedforward_output\n",
        "        X = self.norm3(X)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "4PG5Uc6An8Zj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_layers: int = 6, n_heads: int = 6, d_model: int = 512):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.decoder_layer = DecoderLayer(n_heads, d_model)\n",
        "\n",
        "    def forward(self, X: torch.tensor, encoder_output: torch.tensor) -> torch.tensor:\n",
        "        for i in range(self.n_layers):\n",
        "            X = self.decoder_layer(X, encoder_output)\n",
        "            print(f\"Representation at stage {i}: \\n{X}\\n\")\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "ZqbUqE41n9vu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage example"
      ],
      "metadata": {
        "id": "A7O86-tIup5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"========================== ENCODER ========================================\")\n",
        "batch_size = 1\n",
        "seq_length = 4\n",
        "embed_size = 7\n",
        "sequence_in_matrix_form = torch.rand(batch_size, seq_length, embed_size)\n",
        "print(f\"Representation of the sequence: \\n{sequence_in_matrix_form}\\n\")\n",
        "\n",
        "# Encoder\n",
        "n_layers = 2\n",
        "n_heads = 3\n",
        "d_model = embed_size\n",
        "encoder = Encoder(n_layers=n_layers, n_heads=n_heads, d_model=d_model)\n",
        "encoder_output = encoder(sequence_in_matrix_form)\n",
        "print(f\"Encoder representations: \\n{encoder_output}\\n\\n\")\n",
        "\n",
        "\n",
        "print(\"========================== DECODER ========================================\")\n",
        "batch_size_out = 1\n",
        "seq_length_out = 3\n",
        "embed_size_out = 7\n",
        "sequence_out_in_matrix_form = torch.rand(batch_size_out, seq_length_out, embed_size_out)\n",
        "print(f\"Representation of the produced output sequence: \\n{sequence_out_in_matrix_form}\\n\")\n",
        "\n",
        "# Decoder\n",
        "n_layers = 2\n",
        "n_heads = 3\n",
        "d_model = embed_size_out\n",
        "decoder = Decoder(n_layers=n_layers, n_heads=n_heads, d_model=d_model)\n",
        "decoder_output = decoder(sequence_out_in_matrix_form, encoder_output)\n",
        "print(f\"Decoder representations: \\n{decoder_output}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U7hT5PLsDag",
        "outputId": "07004f79-af83-4101-d8b2-f5c3f4221bf3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================== ENCODER ========================================\n",
            "Representation of the sequence: \n",
            "tensor([[[0.5775, 0.4640, 0.1588, 0.5851, 0.1924, 0.8987, 0.2133],\n",
            "         [0.0191, 0.4507, 0.1655, 0.6276, 0.6418, 0.2010, 0.4140],\n",
            "         [0.4792, 0.2177, 0.1194, 0.7564, 0.1918, 0.1558, 0.2634],\n",
            "         [0.7375, 0.5858, 0.2656, 0.3642, 0.2969, 0.0884, 0.4494]]])\n",
            "\n",
            "Representation at stage 0: \n",
            "tensor([[[ 0.4579,  0.2504, -1.1881,  0.7069, -0.9099,  1.7117, -1.0288],\n",
            "         [-0.9454,  0.6490, -1.4364,  1.6427,  0.6283,  0.1688, -0.7070],\n",
            "         [ 0.8975, -0.2035, -1.0331,  1.9060, -0.9005,  0.1250, -0.7914],\n",
            "         [ 1.8189,  0.8561, -0.8797,  0.4375, -1.1959, -0.4055, -0.6314]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Representation at stage 1: \n",
            "tensor([[[ 0.4266,  0.0532, -1.1480,  0.8267, -1.0182,  1.7351, -0.8754],\n",
            "         [-0.7253,  0.2885, -1.4148,  1.8559,  0.1563,  0.5933, -0.7539],\n",
            "         [ 0.9068, -0.4919, -0.8560,  1.8074, -1.1414,  0.4390, -0.6639],\n",
            "         [ 1.7468,  0.2286, -0.6301,  0.7441, -1.5984,  0.1219, -0.6129]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Encoder representations: \n",
            "tensor([[[ 0.4266,  0.0532, -1.1480,  0.8267, -1.0182,  1.7351, -0.8754],\n",
            "         [-0.7253,  0.2885, -1.4148,  1.8559,  0.1563,  0.5933, -0.7539],\n",
            "         [ 0.9068, -0.4919, -0.8560,  1.8074, -1.1414,  0.4390, -0.6639],\n",
            "         [ 1.7468,  0.2286, -0.6301,  0.7441, -1.5984,  0.1219, -0.6129]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "\n",
            "========================== DECODER ========================================\n",
            "Representation of the produced output sequence: \n",
            "tensor([[[0.8622, 0.2157, 0.0971, 0.6846, 0.9833, 0.5123, 0.1701],\n",
            "         [0.4720, 0.0403, 0.0382, 0.2011, 0.3941, 0.4218, 0.9456],\n",
            "         [0.6031, 0.2926, 0.1719, 0.1431, 0.1404, 0.1578, 0.5813]]])\n",
            "\n",
            "Representation at stage 0: \n",
            "tensor([[[ 1.8521, -0.9451, -1.1614, -0.8681,  0.4205,  0.0765,  0.6255],\n",
            "         [ 1.2145, -0.9301, -0.8303, -1.1528, -0.1740,  0.2817,  1.5912],\n",
            "         [ 1.5724, -0.4344, -0.5141, -1.2945, -0.6001, -0.1493,  1.4200]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Representation at stage 1: \n",
            "tensor([[[ 1.9184, -1.1618, -0.5560, -1.1343,  0.4840,  0.3643,  0.0854],\n",
            "         [ 1.5073, -1.3206, -0.2438, -1.3659, -0.0306,  0.6997,  0.7539],\n",
            "         [ 1.7804, -0.7862,  0.1366, -1.5502, -0.4841,  0.1800,  0.7236]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Decoder representations: \n",
            "tensor([[[ 1.9184, -1.1618, -0.5560, -1.1343,  0.4840,  0.3643,  0.0854],\n",
            "         [ 1.5073, -1.3206, -0.2438, -1.3659, -0.0306,  0.6997,  0.7539],\n",
            "         [ 1.7804, -0.7862,  0.1366, -1.5502, -0.4841,  0.1800,  0.7236]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding"
      ],
      "metadata": {
        "id": "Ac7E5QcMklxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    def forward(self, X: torch.tensor) -> torch.tensor:\n",
        "        PE = torch.ones_like(X, dtype=torch.float32)\n",
        "        d_model = PE.size(-1)\n",
        "        len_seq = PE.size(-2)\n",
        "\n",
        "        for pos in range(len_seq):\n",
        "            for i in range(d_model//2):\n",
        "                exp_term = 2*i / d_model\n",
        "                div_term = 10000 ** exp_term\n",
        "                PE[:, pos, 2*i] = torch.sin(torch.tensor(pos/div_term))\n",
        "                PE[:, pos, 2*i+1] = torch.cos(torch.tensor(pos/div_term))\n",
        "\n",
        "            # Modify the last element explicitly if d_model is odd\n",
        "            if d_model % 2 != 0:\n",
        "                i = d_model // 2\n",
        "                exp_term = 2*i / d_model\n",
        "                div_term = 10000 ** exp_term\n",
        "                PE[:, pos, 2*i] = torch.sin(torch.tensor(pos/div_term))\n",
        "\n",
        "        return PE"
      ],
      "metadata": {
        "id": "hVM1v6ffkeqU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage example"
      ],
      "metadata": {
        "id": "Bk9E8mS3L89F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional encoding matrix\n",
        "pos = PositionalEncoding()\n",
        "\n",
        "# Matrix 1\n",
        "X = torch.tensor([[[1,2,3,4,5], [6,7,8,9,10]]])\n",
        "print(f\"Matrix 1: \\n{X}\")\n",
        "print(f\"Positional encodings: \\n{pos(X)}\")\n",
        "print(f\"Matrix 1 with positional encodings: \\n{X + pos(X)}\\n\")\n",
        "\n",
        "# Matrix 2\n",
        "X = torch.tensor([[[1,2,3], [4,5,6], [7,8,9]]])\n",
        "print(f\"Matrix 2: \\n{X}\")\n",
        "print(f\"Positional encodings: \\n{pos(X)}\")\n",
        "print(f\"Matrix 2 with positional encodings: \\n{X + pos(X)}\\n\")\n",
        "\n",
        "pos1 = pos(X)\n",
        "\n",
        "# Matrix 3\n",
        "X = torch.tensor([[[0,0,0], [1,1,1], [2,2,2]]])\n",
        "print(f\"Matrix 3: \\n{X}\")\n",
        "print(f\"Positional encodings: \\n{pos(X)}\")\n",
        "print(f\"Matrix 3 with positional encodings: \\n{X + pos(X)}\\n\")\n",
        "\n",
        "pos2 = pos(X)\n",
        "\n",
        "# The matrix 2 and matrix 3 have the same positional encodings, since they are based\n",
        "# only on the matrix structure, not on the content\n",
        "print(f\"Are the positional encodings equivalent? {torch.equal(pos1, pos2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLG6BekQU12e",
        "outputId": "4e08d780-ad0d-4cfc-e7f2-325a5f6169cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix 1: \n",
            "tensor([[[ 1,  2,  3,  4,  5],\n",
            "         [ 6,  7,  8,  9, 10]]])\n",
            "Positional encodings: \n",
            "tensor([[[0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
            "         [8.4147e-01, 5.4030e-01, 2.5116e-02, 9.9968e-01, 6.3096e-04]]])\n",
            "Matrix 1 with positional encodings: \n",
            "tensor([[[ 1.0000,  3.0000,  3.0000,  5.0000,  5.0000],\n",
            "         [ 6.8415,  7.5403,  8.0251,  9.9997, 10.0006]]])\n",
            "\n",
            "Matrix 2: \n",
            "tensor([[[1, 2, 3],\n",
            "         [4, 5, 6],\n",
            "         [7, 8, 9]]])\n",
            "Positional encodings: \n",
            "tensor([[[ 0.0000,  1.0000,  0.0000],\n",
            "         [ 0.8415,  0.5403,  0.0022],\n",
            "         [ 0.9093, -0.4161,  0.0043]]])\n",
            "Matrix 2 with positional encodings: \n",
            "tensor([[[1.0000, 3.0000, 3.0000],\n",
            "         [4.8415, 5.5403, 6.0022],\n",
            "         [7.9093, 7.5839, 9.0043]]])\n",
            "\n",
            "Matrix 3: \n",
            "tensor([[[0, 0, 0],\n",
            "         [1, 1, 1],\n",
            "         [2, 2, 2]]])\n",
            "Positional encodings: \n",
            "tensor([[[ 0.0000,  1.0000,  0.0000],\n",
            "         [ 0.8415,  0.5403,  0.0022],\n",
            "         [ 0.9093, -0.4161,  0.0043]]])\n",
            "Matrix 3 with positional encodings: \n",
            "tensor([[[0.0000, 1.0000, 0.0000],\n",
            "         [1.8415, 1.5403, 1.0022],\n",
            "         [2.9093, 1.5839, 2.0043]]])\n",
            "\n",
            "Are the positional encodings equivalent? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wp9lGSH9kDNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uOH1PfZAkDQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}