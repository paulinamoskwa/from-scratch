<p align="center">
<img src='https://github.com/paulinamoskwa/from-scratch/assets/104844027/3cea9cb9-be1f-46ad-b83b-32ca7699a9bc', width=100%/>
</p>

This repo is a collection of all my implementations from scratch of the most popular machine learning models, algorithms, and/or methods.
It contains the ready-to-use codes in the `code` folder, but also a more visually appealing version in the `notebooks` folder.<br>

# Implementations
- Transformer from scratch using PyTorch (*from the paper "Attention is All You Need"*)
  * [Self-Attention](https://github.com/paulinamoskwa/from-scratch/blob/main/code/src/transformer/self_attention.py)
  * [Multi-Head Attention](https://github.com/paulinamoskwa/from-scratch/blob/main/code/src/transformer/multihead_attention.py)
  * [Positional Encoding](https://github.com/paulinamoskwa/from-scratch/blob/main/code/src/transformer/positional_encoding.py)
  * [Encoder](https://github.com/paulinamoskwa/from-scratch/blob/main/code/src/transformer/encoder.py)
  * [Decoder](https://github.com/paulinamoskwa/from-scratch/blob/main/code/src/transformer/decoder.py)
  * Transformer

